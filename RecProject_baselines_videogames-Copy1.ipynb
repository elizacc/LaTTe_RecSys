{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVoK_7xHdaa9",
    "outputId": "5068028f-6526-41db-ab84-9989d1f39bb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting polara\n",
      "  Cloning https://github.com/evfro/polara.git (to revision develop) to /tmp/pip-install-2y70x9ba/polara_606a3c3a2230448b9393ef034c0f5194\n",
      "  Running command git clone -q https://github.com/evfro/polara.git /tmp/pip-install-2y70x9ba/polara_606a3c3a2230448b9393ef034c0f5194\n",
      "  Running command git checkout -b develop --track origin/develop\n",
      "  Switched to a new branch 'develop'\n",
      "  Branch 'develop' set up to track remote branch 'develop' from 'origin'.\n",
      "  Resolved https://github.com/evfro/polara.git to commit 4de4ca7d6f901e32f1e045f190bcb09587162397\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --no-cache-dir --upgrade git+https://github.com/evfro/polara.git@develop#egg=polara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:04:02.081873Z",
     "start_time": "2022-04-30T17:04:02.066353Z"
    },
    "id": "o_HJgdQFdSaf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import polara\n",
    "from polara import get_movielens_data\n",
    "from polara.preprocessing.dataframes import leave_one_out, reindex\n",
    "\n",
    "from dataprep import transform_indices\n",
    "from evaluation import topn_recommendations, downvote_seen_items\n",
    "\n",
    "from polara.lib.tensor import hooi\n",
    "from polara.lib.sparse import tensor_outer_at\n",
    "from polara.evaluation.pipelines import random_grid\n",
    "\n",
    "from sa_hooi import sa_hooi, form_attention_matrix, get_scaling_weights, generate_position_projector\n",
    "\n",
    "from scipy.sparse import csr_matrix, diags\n",
    "from scipy.sparse.linalg import norm, svds\n",
    "from scipy.linalg import solve_triangular, sqrtm\n",
    "\n",
    "from IPython.utils import io\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NbP1tmBdSas"
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:05:21.385576Z",
     "start_time": "2022-04-30T17:05:20.413655Z"
    }
   },
   "outputs": [],
   "source": [
    "col_names = ['userid', 'movieid', 'rating', 'timestamp']\n",
    "data = pd.read_csv(\"ratings_Video_Games.csv\", names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:05:23.333782Z",
     "start_time": "2022-04-30T17:05:23.311780Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def full_preproccessing(data = None):\n",
    "    if (data is None):\n",
    "        data = get_movielens_data(\"ml-10m.zip\", include_time=True)\n",
    "    test_timepoint = data['timestamp'].quantile(\n",
    "    q=0.8, interpolation='nearest'\n",
    "    )\n",
    "    \n",
    "    labels, levels = pd.factorize(data.movieid)\n",
    "    data.movieid = labels\n",
    "\n",
    "    labels, levels = pd.factorize(data.userid)\n",
    "    data.userid = labels\n",
    "    \n",
    "    if (data[\"rating\"].nunique() > 5):\n",
    "        data[\"rating\"] = data[\"rating\"] * 2\n",
    "        \n",
    "    data[\"rating\"] = data[\"rating\"].astype(int)\n",
    "\n",
    "    test_data_ = data.query('timestamp >= @test_timepoint')\n",
    "    train_data_ = data.query(\n",
    "    'userid not in @test_data_.userid.unique() and timestamp < @test_timepoint'\n",
    "    )\n",
    "    \n",
    "    training, data_index = transform_indices(train_data_.copy(), 'userid', 'movieid')\n",
    "    test_data = reindex(test_data_, data_index['items'])\n",
    "\n",
    "    testset_, holdout_ = leave_one_out(\n",
    "    test_data, target='timestamp', sample_top=True, random_state=0\n",
    "    )\n",
    "    testset_valid_, holdout_valid_ = leave_one_out(\n",
    "        testset_, target='timestamp', sample_top=True, random_state=0\n",
    "    )\n",
    "\n",
    "    test_users_val = np.intersect1d(testset_valid_.userid.unique(), holdout_valid_.userid.unique())\n",
    "    testset_valid = testset_valid_.query('userid in @test_users_val').sort_values('userid')\n",
    "    holdout_valid = holdout_valid_.query('userid in @test_users_val').sort_values('userid')\n",
    "\n",
    "    test_users = np.intersect1d(testset_.userid.unique(), holdout_.userid.unique())\n",
    "    testset = testset_.query('userid in @test_users').sort_values('userid')\n",
    "    holdout = holdout_.query('userid in @test_users').sort_values('userid')\n",
    "    \n",
    "    assert holdout_valid.set_index('userid')['timestamp'].ge(\n",
    "        testset_valid\n",
    "        .groupby('userid')\n",
    "        ['timestamp'].max()\n",
    "    ).all()\n",
    "\n",
    "    data_description = dict(\n",
    "        users = data_index['users'].name,\n",
    "        items = data_index['items'].name,\n",
    "        feedback = 'rating',\n",
    "        n_users = len(data_index['users']),\n",
    "        n_items = len(data_index['items']),\n",
    "        n_ratings = training['rating'].nunique(),\n",
    "        min_rating = training['rating'].min(),\n",
    "        test_users = holdout_valid[data_index['users'].name].drop_duplicates().values, # NEW\n",
    "        n_test_users = holdout_valid[data_index['users'].name].nunique() # NEW\n",
    "    )\n",
    "\n",
    "    return training, testset_valid, holdout_valid, testset, holdout, data_description, data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:05:33.317372Z",
     "start_time": "2022-04-30T17:05:24.344646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 36874 invalid observations.\n"
     ]
    }
   ],
   "source": [
    "training, testset_valid, holdout_valid, testset, holdout, data_description, data_index = full_preproccessing(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJOfzHI5dSaz"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:05:36.752963Z",
     "start_time": "2022-04-30T17:05:36.719945Z"
    },
    "code_folding": [
     0,
     52,
     70
    ],
    "id": "nXnDXyWrdSbM"
   },
   "outputs": [],
   "source": [
    "def model_evaluate(recommended_items, holdout, holdout_description, alpha=3, topn=10, dcg=False):\n",
    "    itemid = holdout_description['items']\n",
    "    rateid = holdout_description['feedback']\n",
    "    alpha = 3 if holdout_description[\"n_ratings\"] == 5 else 6\n",
    "    n_test_users = recommended_items.shape[0]\n",
    "    holdout_items = holdout[itemid].values\n",
    "    assert recommended_items.shape[0] == len(holdout_items)\n",
    "    \n",
    "    hits_mask = recommended_items[:, :topn] == holdout_items.reshape(-1, 1)\n",
    "    pos_mask = (holdout[rateid] >= alpha).values\n",
    "    neg_mask = (holdout[rateid] < alpha).values\n",
    "    \n",
    "    # HR calculation\n",
    "    #hr = np.sum(hits_mask.any(axis=1)) / n_test_users\n",
    "    hr_pos = np.sum(hits_mask[pos_mask].any(axis=1)) / n_test_users\n",
    "    hr_neg = np.sum(hits_mask[neg_mask].any(axis=1)) / n_test_users\n",
    "    hr = hr_pos + hr_neg\n",
    "    \n",
    "    # MRR calculation\n",
    "    hit_rank = np.where(hits_mask)[1] + 1.0\n",
    "    mrr = np.sum(1 / hit_rank) / n_test_users\n",
    "    pos_hit_rank = np.where(hits_mask[pos_mask])[1] + 1.0\n",
    "    mrr_pos = np.sum(1 / pos_hit_rank) / n_test_users\n",
    "    neg_hit_rank = np.where(hits_mask[neg_mask])[1] + 1.0\n",
    "    mrr_neg = np.sum(1 / neg_hit_rank) / n_test_users\n",
    "    \n",
    "    # Matthews correlation\n",
    "    TP = np.sum(hits_mask[pos_mask]) # + \n",
    "    FP = np.sum(hits_mask[neg_mask]) # +\n",
    "    cond = (hits_mask.sum(axis = 1) == 0)\n",
    "    FN = np.sum(cond[pos_mask])\n",
    "    TN = np.sum(cond[neg_mask])\n",
    "    N = TP+FP+TN+FN\n",
    "    S = (TP+FN)/N\n",
    "    P = (TP+FP)/N\n",
    "    C = (TP/N - S*P) / np.sqrt(P*S*(1-P)*(1-S))\n",
    "    \n",
    "    # DCG calculation\n",
    "    if dcg:\n",
    "        pos_hit_rank = np.where(hits_mask[pos_mask])[1] + 1.0\n",
    "        neg_hit_rank = np.where(hits_mask[neg_mask])[1] + 1.0\n",
    "        ndcg = np.mean(1 / np.log2(pos_hit_rank+1))\n",
    "        ndcl = np.mean(1 / np.log2(neg_hit_rank+1))\n",
    "    \n",
    "    # coverage calculation\n",
    "    n_items = holdout_description['n_items']\n",
    "    cov = np.unique(recommended_items).size / n_items\n",
    "    if dcg:\n",
    "        return hr, hr_pos, hr_neg, mrr, mrr_pos, mrr_neg, cov, C, ndcg, ndcl\n",
    "    else:\n",
    "        return hr, hr_pos, hr_neg, mrr, mrr_pos, mrr_neg, cov, C\n",
    "\n",
    "def make_prediction(tf_scores, holdout, data_description, mode, context=\"\", print_mode=True):\n",
    "    if (mode and print_mode):\n",
    "        print(f\"for context {context} evaluation ({mode}): \\n\")\n",
    "    for n in [5, 10, 20]:\n",
    "        tf_recs = topn_recommendations(tf_scores, n)\n",
    "        hr, hr_pos, hr_neg, mrr, mrr_pos, mrr_neg, cov, C = model_evaluate(tf_recs, holdout, data_description, topn=n)\n",
    "        if (print_mode):\n",
    "            print(f\"HR@{n} = {hr:.4f}, MRR@{n} = {mrr:.4f}, Coverage@{n} = {cov:.4f}\")\n",
    "            print(f\"HR_pos@{n} = {hr_pos:.4f}, HR_neg@{n} = {hr_neg:.4f}\")\n",
    "            print(f\"MRR_pos@{n} = {mrr_pos:.4f}, MRR_neg@{n} = {mrr_neg:.4f}\")\n",
    "            print(f\"Matthews@{n} = {C:.4f}\")\n",
    "            print(\"-------------------------------------\")\n",
    "        if (n == 10):\n",
    "            mrr10 = mrr\n",
    "            hr10 = hr\n",
    "            c10 = C\n",
    "    return mrr10, hr10, c10\n",
    "\n",
    "def valid_mlrank(mlrank):\n",
    "    '''\n",
    "    Only allow ranks that are suitable for truncated SVD computations\n",
    "    on unfolded compressed tensor (the result of ttm product in HOOI).\n",
    "    '''\n",
    "    #s, r1, r2, r3 = mlrank\n",
    "    s, r1, r3 = mlrank\n",
    "    r2 = r1\n",
    "    #print(s, r1, r2, r3)\n",
    "    return r1*r2 > r3 and r1*r3 > r2 and r2*r3 > r1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EASEr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:05:45.503652Z",
     "start_time": "2022-04-30T17:05:45.483660Z"
    }
   },
   "outputs": [],
   "source": [
    "def matrix_from_observations(data, data_description):\n",
    "    useridx = data[data_description['users']]\n",
    "    itemidx = data[data_description['items']]\n",
    "    values = data[data_description['feedback']]\n",
    "    return csr_matrix((values, (useridx, itemidx)), dtype='f8')\n",
    "\n",
    "\n",
    "def easer(data, data_description, lmbda=500):\n",
    "    X = matrix_from_observations(data, data_description)\n",
    "    G = X.T.dot(X)\n",
    "    diag_indices = np.diag_indices(G.shape[0])\n",
    "    G[diag_indices] += lmbda\n",
    "    P = np.linalg.inv(G.A)\n",
    "    B = P / (-np.diag(P))\n",
    "    B[diag_indices] = 0\n",
    "    \n",
    "    return B\n",
    "\n",
    "def easer_scoring(params, data, data_description):\n",
    "    item_factors = params\n",
    "    test_data = data.assign(\n",
    "        userid = pd.factorize(data['userid'])[0]\n",
    "    )\n",
    "    test_matrix = matrix_from_observations(test_data, data_description)\n",
    "    scores = test_matrix.dot(item_factors)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:27:36.693937Z",
     "start_time": "2022-04-30T17:27:36.682937Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "lambda_grid = np.arange(50, 1000, 50)\n",
    "# lambda_grid = np.arange(5, 55, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:28:03.051324Z",
     "start_time": "2022-04-30T17:27:37.866717Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:25<00:00,  2.52s/it]\n"
     ]
    }
   ],
   "source": [
    "hr_tf = {}\n",
    "mrr_tf = {}\n",
    "C_tf = {}\n",
    "for lmbda in tqdm(lambda_grid):\n",
    "    easer_params = easer(training, data_description, lmbda=lmbda)\n",
    "    easer_scores = easer_scoring(easer_params, testset_valid, data_description)\n",
    "    downvote_seen_items(easer_scores, testset_valid, data_description)\n",
    "    easer_recs = topn_recommendations(easer_scores, topn=10)\n",
    "    hr, hr_pos, hr_neg, mrr, mrr_pos, mrr_neg, cov, C = model_evaluate(easer_recs, holdout_valid, data_description, alpha=3, topn=10, dcg=False)\n",
    "    hr_tf[lmbda] = hr\n",
    "    mrr_tf[lmbda] = mrr\n",
    "    C_tf[lmbda] = C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:28:05.703740Z",
     "start_time": "2022-04-30T17:28:05.696738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 0.07448680351906159\n",
      "40 0.07390029325513198\n",
      "45 0.07390029325513198\n",
      "50 0.07390029325513198\n",
      "30 0.07214076246334311\n"
     ]
    }
   ],
   "source": [
    "hr_sorted = sorted(hr_tf, key=hr_tf.get, reverse=True)\n",
    "for i in range(5):\n",
    "    print(hr_sorted[i], hr_tf[hr_sorted[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:28:06.065338Z",
     "start_time": "2022-04-30T17:28:06.052332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.02702858073825816\n",
      "45 0.026472326956197927\n",
      "40 0.02557161476516315\n",
      "35 0.025290462225946102\n",
      "30 0.025008611460224373\n"
     ]
    }
   ],
   "source": [
    "mrr_sorted = sorted(mrr_tf, key=mrr_tf.get, reverse=True)\n",
    "for i in range(5):\n",
    "    print(mrr_sorted[i], mrr_tf[mrr_sorted[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:28:06.539334Z",
     "start_time": "2022-04-30T17:28:06.521333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 0.08271079069920403\n",
      "5 0.08089233857404816\n",
      "25 0.08057079114741801\n",
      "35 0.0795338071497705\n",
      "20 0.0791246347084103\n"
     ]
    }
   ],
   "source": [
    "C_sorted = sorted(C_tf, key=C_tf.get, reverse=True)\n",
    "for i in range(5):\n",
    "    print(C_sorted[i], C_tf[C_sorted[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:05:47.599608Z",
     "start_time": "2022-04-30T17:05:47.571604Z"
    }
   },
   "outputs": [],
   "source": [
    "data_description = dict(\n",
    "    users = data_index['users'].name,\n",
    "    items = data_index['items'].name,\n",
    "    feedback = 'rating',\n",
    "    n_users = len(data_index['users']),\n",
    "    n_items = len(data_index['items']),\n",
    "    n_ratings = training['rating'].nunique(),\n",
    "    min_rating = training['rating'].min(),\n",
    "    test_users = holdout[data_index['users'].name].drop_duplicates().values,\n",
    "    n_test_users = holdout[data_index['users'].name].nunique()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EASEr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-30T17:11:42.511130Z",
     "start_time": "2022-04-30T17:11:40.343807Z"
    }
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 14.2 GiB for an array with shape (43720, 43720) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m easer_params \u001b[38;5;241m=\u001b[39m \u001b[43measer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_description\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m easer_scores \u001b[38;5;241m=\u001b[39m easer_scoring(easer_params, testset, data_description)\n\u001b[0;32m      3\u001b[0m downvote_seen_items(easer_scores, testset, data_description)\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36measer\u001b[1;34m(data, data_description, lmbda)\u001b[0m\n\u001b[0;32m     11\u001b[0m diag_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag_indices(G\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     12\u001b[0m G[diag_indices] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m lmbda\n\u001b[1;32m---> 13\u001b[0m P \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(\u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA\u001b[49m)\n\u001b[0;32m     14\u001b[0m B \u001b[38;5;241m=\u001b[39m P \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mdiag(P))\n\u001b[0;32m     15\u001b[0m B[diag_indices] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py:675\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr):\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 675\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    677\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py:1039\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1039\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py:1202\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 14.2 GiB for an array with shape (43720, 43720) and data type float64"
     ]
    }
   ],
   "source": [
    "easer_params = easer(training, data_description, lmbda=C_sorted[i])\n",
    "easer_scores = easer_scoring(easer_params, testset, data_description)\n",
    "downvote_seen_items(easer_scores, testset, data_description)\n",
    "\n",
    "make_prediction(easer_scores, holdout, data_description, mode='Test')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CoFFeeProject.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "009c838d92940ae6fa3c0eca0f0908a58be7fe030119f0cd30e204cb459dcff7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.306px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
